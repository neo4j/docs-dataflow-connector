= Prerequisites


// tag::neo4j[]
== Neo4j instance

You need a running Neo4j instance which the data can flow into.

If you don't have an instance yet, you have two options:

- sign-up for a free link:https://neo4j.com/cloud/aura-free/[AuraDB] instance
- install and self-host Neo4j in a location that is publicly accessible (see link:https://neo4j.com/docs/operations-manual/current/installation/[Neo4j -> Installation]) with port 7687 open (Bolt protocol)

Either way, you then need to create a file containing your database connection information in JSON format.
We will refer to this file as `neo4j-connection-info.json`.
The file can be uploaded either as a secret to Google Cloud Secret Manager or directly into a Google Cloud Storage bucket.

[.tabbed-example]
====
[.include-with-basic-auth]
=====
The basic authentication scheme relies on traditional username and password.
This scheme can also be used to authenticate against an LDAP server.

[source,json]
----
{
  "server_url": "neo4j+s://xxxx.databases.neo4j.io",
  "database": "neo4j",
  "username": "<username>",
  "pwd": "<password>"
}
----
=====

[.include-with-no-auth]
=====
If authentication is disabled on the server, credentials can be omitted.

[source,json]
----
{
  "server_url": "neo4j+s://xxxx.databases.neo4j.io",
  "database": "neo4j",
  "auth_type": "none"
}
----
=====

[.include-with-kerberos-auth]
=====
The Kerberos authentication scheme requires a base64-encoded ticket.
It can only be used if the server has the link:{neo4j-docs-base-uri}/kerberos-add-on/current/deployment/[Kerberos Add-on installed].

[source,json]
----
{
  "server_url": "neo4j+s://xxxx.databases.neo4j.io",
  "database": "neo4j",
  "auth_type": "kerberos",
  "ticket": "<base 64 encoded Kerberos ticket>"
}
----
=====

[.include-with-bearer-auth]
=====
The bearer authentication scheme requires a base64-encoded token provided by an Identity Provider through Neo4j's link:{neo4j-docs-base-uri}/operations-manual/current/authentication-authorization/sso-integration[Single Sign-On feature].

[source,json]
----
{
  "server_url": "neo4j+s://xxxx.databases.neo4j.io",
  "database": "neo4j",
  "auth_type": "bearer",
  "token": "<bearer token>"
}
----
=====

[.include-with-custom-auth]
=====
To log into a server having a custom authentication scheme.

[source,json]
----
{
  "server_url": "neo4j+s://xxxx.databases.neo4j.io",
  "database": "neo4j",
  "auth_type": "custom",
  "principal": "<principal>",
  "credentials": "<credentials>",
  "realm": "<realm>",
  "scheme": "<scheme>",
  "parameters": {"<key>": "<value>"}
}
----
=====
====

// end::neo4j[]


// tag::secret[]
== Google Secret Manager

If you wish to store the credentials file as a Google secret, you need access to link:https://console.cloud.google.com/security/secret-manager[Google Secret Manager].

Go ahead and create a new secret and upload the `neo4j-connection-info.json` file as value.
// end::secret[]

// tag::google-cloud-storage[]
== Google Cloud Storage bucket

You need a link:https://console.cloud.google.com/storage/[Google Cloud Storage] bucket.
This is the one and only location from where the Dataflow job can source files (both configuration files and source CSVs, if any).

Unless you stored the credentials file in Google Secret Manager, go ahead and upload the `neo4j-connection-info.json` file to your Cloud Storage bucket.
// end::google-cloud-storage[]


// tag::dataflow-job[]
== Google Dataflow job

The link:https://console.cloud.google.com/dataflow[Google Dataflow] job glues all the pieces together and performs the data import.
All the work that is now needed is to craft a _job specification_ file to provide Dataflow with all the information it needs to load the data into Neo4j.

[.shadow]
image::{common}:image$google-dataflow.jpg[width=400]


[NOTE]
All Google-related resources (Cloud project, Cloud Storage buckets, Dataflow job) should either belong to the same account, or to one which the Dataflow job has permissions to access.
// end::dataflow-job[]
